structure of the project: 
utilfuns/ 
    data_capture : load data from Yahoo finance. 
    data_processing : load from file and normalization .
    plot_eachclass : plot 30 prices from each clusters.
pycluster_clustering/
    kmeans.py
    kmedoids.py
    tree.py
    som.p!
Fri Jun 28 09:18:10 EDT 2013
---------------------------------------------
Sat Jun 22 21:32:59 EDT 2013
In data_capture.py: I read ticker symbols from the Yahoo finance, then save them into local files, i.e. valid_labels, valid_prices.
parameters might be changed: startdate, stopdate, numberofprices.

In data_process.py: I randomly plot N stock markets and their labels, save to local .png image.two images, ...before... and ...after... shows the effect of normalization. 
Be cautious that some data are entirely 0s, which caused me some trouble when I normalize

---------------------------------------------
Sun Jun 23 09:26:40 EDT 2013
add kmeans_euclidean() function that groups prices based on the euclidean distance. 
---------------------------------------------
Tue Jun 25 22:03:03 EDT 2013
With the help of pycluster, three metrics for both kmeans and kmedoids: 
euclidean ~30s 
spearman's R ~3-4min 
Kendall's tau ~10 - 15min . 

Note for kmeans, npass = 5, kmedoids npass=1
ncluster=10 for both.  
---------------------------------------------
Wed Jun 26 09:55:40 EDT 2013
start using pycluster for hierarchical clustering. 
The eculidean distance isn't good, ncluster = 10, there is only one super large cluster. 
use spearman's R: 
method s: not good, skew. 
m: pretty good, no skew.
a: minor skew, there are few large clusters.

use kendall's tau:
s: skew, one class is as large as ~2950
m: not bad, no skew. 
a: skew.

obviously, spearman, kendall, with m: max linkage is the best. 
I also did ward linkage using scikit toolbox. the performace is also no-skew.


Wed Jun 26 23:05:05 EDT 2013
change the structure of code, now they run in a loop through all the methods
